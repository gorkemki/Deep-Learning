{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gorkem/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, defaultdict\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Gorkem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to read data: 0.010062408000003131 seconds\n",
      "Time to preprocess 2951 data points: 1.017363693 seconds\n",
      "Time to preprocess 1266 data points: 0.37066819600000045 seconds\n",
      "Top 10 most common words are: [('eur', 900), ('company', 490), ('mn', 440), ('finnish', 320), ('sales', 307), ('said', 306), ('profit', 298), ('million', 292), ('net', 280), ('year', 262)]\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.31106001  0.47663999  0.12986    ...  0.073349    0.27316001\n",
      "  -0.79343998]\n",
      " [ 0.60631001 -0.37365001 -0.31874001 ... -0.35591999  1.00189996\n",
      "   0.14789   ]\n",
      " ...\n",
      " [ 0.093759   -0.025291    0.15722001 ...  0.59522998  0.23089001\n",
      "   0.26718   ]\n",
      " [-0.10076     0.64577001 -0.081351   ... -0.15046     0.074236\n",
      "  -0.037193  ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Total vocabulary size=  6683\n",
      "Number of words found=  5925\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 24, 100)           668300    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 96)                75648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 291       \n",
      "=================================================================\n",
      "Total params: 744,239\n",
      "Trainable params: 75,939\n",
      "Non-trainable params: 668,300\n",
      "_________________________________________________________________\n",
      "Train on 2951 samples, validate on 1266 samples\n",
      "Epoch 1/8\n",
      "2951/2951 [==============================] - 2s 614us/step - loss: 0.8466 - accuracy: 0.6347 - val_loss: 0.7598 - val_accuracy: 0.6793\n",
      "Epoch 2/8\n",
      "2951/2951 [==============================] - 1s 350us/step - loss: 0.7038 - accuracy: 0.6933 - val_loss: 0.7023 - val_accuracy: 0.6919\n",
      "Epoch 3/8\n",
      "2951/2951 [==============================] - 1s 369us/step - loss: 0.6332 - accuracy: 0.7282 - val_loss: 0.6574 - val_accuracy: 0.7267\n",
      "Epoch 4/8\n",
      "2951/2951 [==============================] - 1s 378us/step - loss: 0.5621 - accuracy: 0.7645 - val_loss: 0.6415 - val_accuracy: 0.7354\n",
      "Epoch 5/8\n",
      "2951/2951 [==============================] - 1s 405us/step - loss: 0.5065 - accuracy: 0.7963 - val_loss: 0.6333 - val_accuracy: 0.7441\n",
      "Epoch 6/8\n",
      "2951/2951 [==============================] - 1s 434us/step - loss: 0.4728 - accuracy: 0.8065 - val_loss: 0.6159 - val_accuracy: 0.7504\n",
      "Epoch 7/8\n",
      "2951/2951 [==============================] - 1s 386us/step - loss: 0.4199 - accuracy: 0.8309 - val_loss: 0.6507 - val_accuracy: 0.7520\n",
      "Epoch 8/8\n",
      "2951/2951 [==============================] - 1s 385us/step - loss: 0.3971 - accuracy: 0.8455 - val_loss: 0.6299 - val_accuracy: 0.7607\n",
      "1266/1266 [==============================] - 0s 256us/step\n",
      "Test accuracy is 76.07%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nKeras LSTM Model Report:\\n\\nTime to read data: 0.012889714000039021 seconds\\nTime to preprocess 2951 data points: 0.77431815500006 seconds\\nTime to preprocess 1266 data points: 0.3313503300000775 seconds\\nTop 10 most common words are: [(\\'eur\\', 900), (\\'company\\', 490), (\\'mn\\', 440), (\\'finnish\\', 320), (\\'sales\\', 307), (\\'said\\', 306), (\\'profit\\', 298), (\\'million\\', 292), (\\'net\\', 280), (\\'year\\', 262)]\\nModel: \"sequential_10\"\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #   \\n=================================================================\\nembedding_10 (Embedding)     (None, 24, 100)           668300    \\n_________________________________________________________________\\nlstm_10 (LSTM)               (None, 96)                75648     \\n_________________________________________________________________\\ndropout_10 (Dropout)         (None, 96)                0         \\n_________________________________________________________________\\ndense_10 (Dense)             (None, 3)                 291       \\n=================================================================\\nTotal params: 744,239\\nTrainable params: 75,939\\nNon-trainable params: 668,300\\n_________________________________________________________________\\nTrain on 2951 samples, validate on 1266 samples\\nEpoch 1/8\\n2951/2951 [==============================] - 1s 427us/step - loss: 0.8437 - accuracy: 0.6384 - val_loss: 0.7535 - val_accuracy: 0.6809\\nEpoch 2/8\\n2951/2951 [==============================] - 1s 286us/step - loss: 0.6965 - accuracy: 0.7055 - val_loss: 0.6769 - val_accuracy: 0.7101\\nEpoch 3/8\\n2951/2951 [==============================] - 1s 282us/step - loss: 0.6195 - accuracy: 0.7404 - val_loss: 0.6490 - val_accuracy: 0.7267\\nEpoch 4/8\\n2951/2951 [==============================] - 1s 279us/step - loss: 0.5730 - accuracy: 0.7604 - val_loss: 0.6422 - val_accuracy: 0.7472\\nEpoch 5/8\\n2951/2951 [==============================] - 1s 271us/step - loss: 0.5146 - accuracy: 0.7899 - val_loss: 0.6201 - val_accuracy: 0.7559\\nEpoch 6/8\\n2951/2951 [==============================] - 1s 282us/step - loss: 0.4671 - accuracy: 0.8113 - val_loss: 0.6138 - val_accuracy: 0.7528\\nEpoch 7/8\\n2951/2951 [==============================] - 1s 271us/step - loss: 0.4259 - accuracy: 0.8319 - val_loss: 0.6308 - val_accuracy: 0.7551\\nEpoch 8/8\\n2951/2951 [==============================] - 1s 277us/step - loss: 0.4067 - accuracy: 0.8370 - val_loss: 0.6124 - val_accuracy: 0.7717\\n1266/1266 [==============================] - 0s 155us/step\\nTest accuracy is 77.17%\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pandas\n",
    "import re\n",
    "import string\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from numpy import zeros\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import collections\n",
    "NB_WORDS = 5000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "EPOCHS = 8 # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 96  # Size of the batches used in the mini-batch gradient descent\n",
    "\n",
    "def read_data():\n",
    "    \"\"\"\n",
    "    Read train/test data\n",
    "\n",
    "    Returns:\n",
    "    train -- Training dataframe, with 'sentence' and 'label'\n",
    "    test -- Testing dataframe, with 'sentence' and 'label'\n",
    "    \"\"\"\n",
    "    start = timer()\n",
    "    train = pandas.read_csv(\"training_data.csv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "    \n",
    "    test = pandas.read_csv(\"testing_data.csv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "    end = timer()\n",
    "    print (\"Time to read data: {} seconds\".format(end - start))\n",
    "    \n",
    "    sns.countplot(x='label', data=train)\n",
    "    return train,test\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess data:\n",
    "    -- strip spaces and lowercase\n",
    "    -- get rid of numbers\n",
    "    -- get rid of punctuations\n",
    "    -- get rid of single letters\n",
    "\n",
    "    Keyword arguments:\n",
    "    df -- Input two column dataframe with 'sentence' and 'label'\n",
    "\n",
    "    Returns:\n",
    "    df -- With preprocessed 'sentence' column and 'label'\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocessor(sentence):\n",
    "        sentence = sentence.strip().lower()\n",
    "        sentence = re.sub(r\"\\d+\", \"\", sentence)\n",
    "        sentence = sentence.translate(sentence.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        sentence = \" \".join([w for w in nltk.word_tokenize(sentence) if len(w) > 1])\n",
    "\n",
    "        return sentence\n",
    "    \n",
    "    def remove_stopwords(input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def remove_mentions(input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "       \n",
    "   \n",
    "    start = timer()\n",
    "    df[\"sentence\"] = df[\"sentence\"].apply(preprocessor)\n",
    "    df[\"sentence\"] = df[\"sentence\"].apply(remove_stopwords).apply(remove_mentions)\n",
    "    \n",
    "    end = timer()\n",
    "    print (\"Time to preprocess {} data points: {} seconds\".format(df.shape[0], end - start))\n",
    "    return df\n",
    "\n",
    "def train_eval_clf(train_data,test_data):\n",
    "    \"\"\"\n",
    "    Train classifier\n",
    "    -- logistic regression\n",
    "    -- with TFIDF features\n",
    "\n",
    "    Keyword arguments:\n",
    "    df -- Input two column dataframe with 'sentence' and 'label'\n",
    "\n",
    "    Returns:\n",
    "    vec -- Fitted vectorizer\n",
    "    clf -- Fitted classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NB_WORDS)\n",
    "    X_train= train_data[\"sentence\"]\n",
    "    y_train=train_data[\"label\"]\n",
    "    X_test= test_data[\"sentence\"]\n",
    "    y_test=test_data[\"label\"]\n",
    "    \n",
    "    \n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    print('Top 10 most common words are:', collections.Counter(tokenizer.word_counts).most_common(10))\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    #print(\"word_index=\",word_index)\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "   \n",
    "    MAX_LEN = 24\n",
    "    \n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN)\n",
    "   \n",
    "    le = LabelEncoder()\n",
    "    y_train_le = le.fit_transform(y_train)\n",
    "    y_test_le = le.transform(y_test)\n",
    "    y_train_oh = to_categorical(y_train_le)\n",
    "    y_test_oh = to_categorical(y_test_le)\n",
    "    \n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open('glove.6B.100d.txt')\n",
    "    for line in f:\n",
    "        \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    k=0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            k+=1\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(embedding_matrix)\n",
    "    print(\"Total vocabulary size= \",vocab_size)\n",
    "    print(\"Number of words found= \",k)\n",
    "    \n",
    "    glove_model = models.Sequential()\n",
    "    glove_model.add(layers.Embedding(len(word_index) + 1, 100, input_length=MAX_LEN, weights=[embedding_matrix],trainable=False))\n",
    "    glove_model.add(layers.LSTM(96))\n",
    "    glove_model.add(layers.Dropout(0.20))\n",
    "    glove_model.add(layers.Dense(3, activation='softmax'))\n",
    "    glove_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    glove_model.summary()\n",
    "\n",
    "\n",
    "    glove_history = glove_model.fit(X_train_pad, y_train_oh, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                              validation_data=(X_test_pad, y_test_oh))\n",
    "    glove_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    test_model = glove_model.evaluate(X_test_pad, y_test_oh)\n",
    "    print('Test accuracy is {0:.2f}%'.format(test_model[1] * 100))\n",
    "    \n",
    "\n",
    "    end = timer()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train, test = read_data()\n",
    "    train = preprocess_data(train)\n",
    "    test = preprocess_data(test)\n",
    "    train_eval_clf(train,test)\n",
    "   \n",
    "\n",
    "\"\"\"\n",
    "Keras LSTM Model Report:\n",
    "\n",
    "Time to read data: 0.012889714000039021 seconds\n",
    "Time to preprocess 2951 data points: 0.77431815500006 seconds\n",
    "Time to preprocess 1266 data points: 0.3313503300000775 seconds\n",
    "Top 10 most common words are: [('eur', 900), ('company', 490), ('mn', 440), ('finnish', 320), ('sales', 307), ('said', 306), ('profit', 298), ('million', 292), ('net', 280), ('year', 262)]\n",
    "Model: \"sequential_10\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_10 (Embedding)     (None, 24, 100)           668300    \n",
    "_________________________________________________________________\n",
    "lstm_10 (LSTM)               (None, 96)                75648     \n",
    "_________________________________________________________________\n",
    "dropout_10 (Dropout)         (None, 96)                0         \n",
    "_________________________________________________________________\n",
    "dense_10 (Dense)             (None, 3)                 291       \n",
    "=================================================================\n",
    "Total params: 744,239\n",
    "Trainable params: 75,939\n",
    "Non-trainable params: 668,300\n",
    "_________________________________________________________________\n",
    "Train on 2951 samples, validate on 1266 samples\n",
    "Epoch 1/8\n",
    "2951/2951 [==============================] - 1s 427us/step - loss: 0.8437 - accuracy: 0.6384 - val_loss: 0.7535 - val_accuracy: 0.6809\n",
    "Epoch 2/8\n",
    "2951/2951 [==============================] - 1s 286us/step - loss: 0.6965 - accuracy: 0.7055 - val_loss: 0.6769 - val_accuracy: 0.7101\n",
    "Epoch 3/8\n",
    "2951/2951 [==============================] - 1s 282us/step - loss: 0.6195 - accuracy: 0.7404 - val_loss: 0.6490 - val_accuracy: 0.7267\n",
    "Epoch 4/8\n",
    "2951/2951 [==============================] - 1s 279us/step - loss: 0.5730 - accuracy: 0.7604 - val_loss: 0.6422 - val_accuracy: 0.7472\n",
    "Epoch 5/8\n",
    "2951/2951 [==============================] - 1s 271us/step - loss: 0.5146 - accuracy: 0.7899 - val_loss: 0.6201 - val_accuracy: 0.7559\n",
    "Epoch 6/8\n",
    "2951/2951 [==============================] - 1s 282us/step - loss: 0.4671 - accuracy: 0.8113 - val_loss: 0.6138 - val_accuracy: 0.7528\n",
    "Epoch 7/8\n",
    "2951/2951 [==============================] - 1s 271us/step - loss: 0.4259 - accuracy: 0.8319 - val_loss: 0.6308 - val_accuracy: 0.7551\n",
    "Epoch 8/8\n",
    "2951/2951 [==============================] - 1s 277us/step - loss: 0.4067 - accuracy: 0.8370 - val_loss: 0.6124 - val_accuracy: 0.7717\n",
    "1266/1266 [==============================] - 0s 155us/step\n",
    "Test accuracy is 77.17%\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
